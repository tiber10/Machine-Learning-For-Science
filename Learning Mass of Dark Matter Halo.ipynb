{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific Test III. Learning Mass of Dark Matter Halo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Using the provided dataset implement a regression algorithm to learn the mapping between lensing images and the lensing dark matter halo mass. You can use the machine learning algorithm of your choice. Please implement your approach in PyTorch and discuss your strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataSet Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and setup the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cpu\")\n",
    "path = 'F:/mini projet/dataLR2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class contains the important methods for data loading, converting to tensors, and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "[array([[34, 29, 25, ..., 32, 31, 35],\n",
      "       [36, 30, 39, ..., 31, 30, 39],\n",
      "       [33, 36, 29, ..., 32, 27, 27],\n",
      "       ...,\n",
      "       [29, 27, 31, ..., 33, 27, 35],\n",
      "       [39, 36, 25, ..., 22, 20, 29],\n",
      "       [34, 19, 23, ..., 29, 25, 18]], dtype=int64)\n",
      " 237.59861225420505]\n"
     ]
    }
   ],
   "source": [
    "class Data():\n",
    "    def __init__(self):\n",
    "        super(Data, self).__init__()\n",
    "        print(\"DONE\")\n",
    "        \n",
    "    def loadData(self):\n",
    "        listdirectory = os.listdir(path=path)\n",
    "        dataLR = np.load(os.path.join(path, listdirectory[0]), allow_pickle=True)\n",
    "        return dataLR\n",
    "        \n",
    "    def splittingData(self):\n",
    "        frames = []\n",
    "        labels = []\n",
    "        for count, filename in enumerate(os.listdir(path)): \n",
    "            data = np.load(path+'/'+filename,allow_pickle=True)\n",
    "            frames.append(data[:1][0])  \n",
    "            labels.append(data[1:2][0]) \n",
    "        return frames,labels  \n",
    "          \n",
    "    def converting(self):\n",
    "        frames,labels = obj.splittingData()\n",
    "        frames = np.array(frames)\n",
    "        labels = np.array(labels)\n",
    "        return frames/255.0, labels\n",
    "    \n",
    "    def tesTrainSplit(self):\n",
    "        X,y = self.converting()\n",
    "        fr_tr, fr_te, lab_tr, lab_te = train_test_split(X, y, test_size=0.1, random_state=9)\n",
    "        fr_tr = torch.tensor(fr_tr)\n",
    "        fr_te = torch.tensor(fr_te)\n",
    "        lab_tr = torch.tensor(lab_tr)\n",
    "        lab_te = torch.tensor(lab_te)\n",
    "        fr_tr = torch.unsqueeze(fr_tr, 1)\n",
    "        fr_te = torch.unsqueeze(fr_te, 1)\n",
    "        return fr_tr,fr_te,lab_tr,lab_te\n",
    "    \n",
    "    def tensorData(self):\n",
    "        fr_tr,fr_te,lab_tr,lab_te = self.tesTrainSplit()\n",
    "        train = torch.utils.data.TensorDataset(fr_tr, lab_tr)\n",
    "        test = torch.utils.data.TensorDataset(fr_te, lab_te)\n",
    "        train_loader = torch.utils.data.DataLoader(train, batch_size=64)\n",
    "        test_loader = torch.utils.data.DataLoader(test, batch_size=64)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "obj = Data()\n",
    "data = obj.loadData()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames,labels = obj.splittingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_tr,fr_te,lab_tr,lab_te = obj.tesTrainSplit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, i will create the loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = obj.tensorData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, im defining the following architecture for my model\n",
    "\n",
    "Conv 2D ==> Conv 2D ==> Conv 2D ==> Batch Normalization ==> Max Pooling ==> Fully Connected ==> FC => Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copirights Pytorch Official Website\n",
    "class Regress(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regress, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3)\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(3, 2, padding=1)\n",
    "        self.fc1 = nn.Linear(32*36*36, 500)\n",
    "        self.fc2 = nn.Linear(500, 500)\n",
    "        self.fc3 = nn.Linear(500, 1)\n",
    "           \n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.pool(F.relu(self.conv1(x))))\n",
    "        x = self.bn(self.pool(F.relu(self.conv2(x))))\n",
    "        x = self.bn(self.pool(F.relu(self.conv3(x))))\n",
    "        x = x.view(-1, 32*36*36)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = Regress().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "epochs = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function for evaluating our model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in data_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss += F.cross_entropy(output, target, size_average=False).data[0]\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "        \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyrights Kaggle\n",
    "def train():\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device).view(labels.shape[0], 1)\n",
    "    outputs = model(inputs.float())\n",
    "    loss = metric(outputs, labels.float())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "    loop.set_postfix(loss = running_loss/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training with 32 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    loop = tqdm(train_loader, position=0)\n",
    "    for inputs, label in loop:\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device).view(label.shape[0], 1)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = metric(outputs, labels.float())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n",
    "        loop.set_postfix(loss = running_loss/len(train_loader))\n",
    "    train_losses.append(running_loss/len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Method for making predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediciton(data_loader):\n",
    "    model.eval()\n",
    "    test_pred = torch.LongTensor()\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        data = Variable(data, volatile=True)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            \n",
    "        output = model(data)\n",
    "        \n",
    "        pred = output.cpu().data.max(1, keepdim=True)[1]\n",
    "        test_pred = torch.cat((test_pred, pred), dim=0)\n",
    "        \n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
